'''
# Qラーニングによるネズミ学習問題

## ネズミ学習問題
かごに入った一匹のネズミが餌をとる手順を学習したい
餌は自販機から出てくる
・自販機の電源ボタンを押すとON/OFFが切り替わる
・自販機がONの場合に商品ボタンを押すと餌が出てくる

## Qラーニング
状態(s)の数×行動(a)の選択肢 のテーブルを用意し、その中に行動の価値(Q値)を入れていく
望ましい行動に関するテーブルに報酬を加算していくことで学習が進む
QラーニングにおけるQ値の更新処理は一般に以下の式で表される
    Q(s[t], a[t]) <- (1 - α)・Q(s[t], a[t]) + α・(r[t+1] + γ・max(Q[t+1]))
    ・r[t+1]: 状態変化による報酬値
    ・α: 学習率（0～1）。Q値の更新をどれだけ急激に行うかを表す
    ・γ: 割引率（0～1）。状態変化後の価値をどれだけ割り引いて考えるかのパラメータ
学習が進むと、望ましい行動のQ値が高くなっていく

## ネズミ学習問題のQラーニング
ある時点tにおいて、電源がOFFならt[t]=0、ONならs[t]=1とし、
その時の行動として、電源ボタンを押すならa[t]=0、商品ボタンを押すならa[t]=1とする
その場合、Q(s,a)は以下のようになる
    ・Q(0,0): 状態s=0(電源OFF)のとき行動a=0(電源ボタンを押す)
    ・Q(0,1): 状態s=0(電源OFF)のとき行動a=1(商品ボタンを押す)
    ・Q(1,0): 状態s=1(電源ON)のとき行動a=0(電源ボタンを押す)
    ・Q(1,1): 状態s=1(電源ON)のとき行動a=1(商品ボタンを押す)

人間がこの問題を解くなら直感的に、状態1のときに行動1をとるのが最善であると考え、
状態1にするために、状態0のときは行動0をとれば良いと分かる
これをランダムな行動によって学習させることができれば、ネズミでもこの問題が解けることになる

まずQテーブルの初期値を0とする
最初の行動として、状態0のとき行動0をとったときのQ値の変遷を考える
この行動により状態は1に変遷するため、学習率0.5、割引率0.9とするとQ値更新式は、
    Q(0,0) <- (1 - 0.5)・Q(0,0) + 0.5・(0 + 0.9・max(Q(1,a)))) = 0
この行動によって餌は出てこないため、r=0としている
また、Q(1,0), Q(1,1)ともに0であるためmax(Q(1,a))も0としている
この結果、Qテーブルは以下のように更新される
    Q(s,a)  更新前  状態0で行動0をとったときの更新後
    Q(0,0)    0       0 <- 更新
    Q(0,1)    0       0
    Q(1,0)    0       0
    Q(1,1)    0       0

その後、次の行動として行動1をとるとすると、状態は1になっているため、餌が出てくる
したがって、r=1として
    Q(1,1) <- (1 - 0.5)・Q(1,1) + 0.5・(1 + 0.9・max(Q(1,a))) = 0.5
となるため、Qテーブルは以下のように更新される
    Q(s,a)  更新前  状態1で行動1をとったときの更新後
    Q(0,0)    0       0
    Q(0,1)    0       0
    Q(1,0)    0       0
    Q(1,1)    0       0.5 <- 更新

問題設定によっては状態1のまま続けることも可能だが、今回は、報酬を得た後は初期状態に戻るとする
初期状態0に戻り、再び行動0をとった場合のQ値の変遷を考える
最初の時とは異なり、Q(1,1)が0.5になっているため、max(Q(1,a))が0.5になる点に注意すると
    Q(0,0) <- (1 -0.5)・Q(0,0) + 0.5・(0 + 0.9・0.5) = 0.225
となり、Qテーブルは以下のように更新される
    Q(s,a)  更新前  状態0で行動0をとったときの更新後
    Q(0,0)    0       0.225 <- 更新
    Q(0,1)    0       0
    Q(1,0)    0       0
    Q(1,1)    0       0.5

この辺りで、人間が直感的にわかる価値基準とほぼ同じようなQテーブルが出来上がる

## ε-グリーディー法
上記のようにランダムな行動の結果に対して報酬を与えていくのがQラーニングだが、
行動を完全にランダムにしてしまうと、無意味な行動が増え、学習コストがかさむ
そこで、一定の確率でランダムに行動し、それ以外はQテーブルに従った行動をとるようにすると効率が良い
この手法をε-グリーディー法と呼ぶ
'''
import cupy as cp

max_steps = 5 # 1試行あたり最大何回行動させるか
episodes = 10 # 試行回数
q_table = cp.zeros((2, 2)) # 状態数2×行動選択肢2のQテーブル

# ε-グリーディー法により次の行動を決定
def get_action(next_state, episode):
    epsion = 0.5 * (0.99 ** episode) # 徐々にεを小さくする＝最適行動をとる確率を上げる
    if epsion <= cp.random.uniform(0, 1):  # εが0～1の一様乱数以下なら最適行動をとる
        next_action = cp.argmax(q_table[next_state]) # Q[t+1]が最大のインデックス(=行動)
    else: # それ以外は乱数行動
        next_action = cp.random.randint(2)
    return next_action

# ネズミに一回行動させ、状態を一つ進める
## (次の状態, 報酬)が返る
def step(state, action):
    reward = 0
    if state == 0: # 電源OFFのとき
        if action == 0: # 電源ボタンを押せば
            state = 1 # 電源ONにする
        else: # 商品ボタンを押しても何も起こらない
            pass
    else: # 電源ONのとき
        if action == 0: # 電源ボタンを押せば
            state = 0 # 電源OFFにする
        else: # 商品ボタンを押せば餌がもらえる
            reward = 1
    return state, reward

# Qテーブルの更新処理
def update_Qtable(state, action, reward, next_state):
    alpha, gamma = 0.5, 0.9
    # Q(s[t], a[t]) <- (1 - α)・Q(s[t], a[t]) + α・(r[t+1] + γ・max(Q[t+1]))
    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * max(q_table[next_state]))

# メイン処理
for episode in range(episodes):
    state, episode_reward = 0, 0
    
    for t in range(max_steps): # 1試行の行動ループ
        action = get_action(state, episode) # a[t+1]選択
        next_state, reward = step(state, action) # ネズミ行動
        #print('state: %d, action: %d, next_state: %d, reward: %d' % (state, action, next_state, reward))
        episode_reward += reward # 1試行中に与えられた報酬を合算
        update_Qtable(state, action, reward, next_state) # 行動の結果からQテーブル更新
        state = next_state # 状態を次の状態へ更新
    
    print('episode %d: total reward %d' % (episode+1, episode_reward))
    print(q_table)

'''
# 結果
Qテーブル
[
           行動0   行動1
    状態0 [Q(0,0) Q(0,1)]
    状態1 [Q(1,0) Q(1,1)]
]
が、学習が進むごとに更新されていき、最終的に
[
           行動0   行動1
    状態0 [  中     小  ]
    状態1 [  小     大  ]
]
となれば学習は成功

今回は、
[
    [5.86825162 2.60025302]
    [3.58372529 7.36479906]
]
となり、ネズミは餌の取り方を学習することができた
'''